#!/usr/bin/env python3
"""
Ollama è¯Šæ–­è„šæœ¬ - æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦æ­£å¸¸å·¥ä½œ
ä½¿ç”¨æ–¹æ³•: python backend/scripts/diagnose_ollama.py
"""
import httpx
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.settings import get_settings

def check_ollama_service():
    """æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€"""
    print("=" * 60)
    print("ğŸ” Ollama è¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    settings = get_settings()
    # Normalize to use 127.0.0.1 instead of localhost to avoid IPv6 issues
    base_url = settings.ollama_api_url.replace("/api/generate", "").replace("localhost", "127.0.0.1")
    api_url = settings.ollama_api_url.replace("localhost", "127.0.0.1")
    
    print(f"\nğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   API URL: {settings.ollama_api_url}")
    print(f"   æ¨¡å‹: {settings.ollama_model}")
    print(f"   è¶…æ—¶: {settings.ollama_timeout}ç§’")
    print(f"   åŸºç¡€ URL: {base_url}")
    
    issues = []
    
    # æ­¥éª¤ 1: æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
    print(f"\n[1/4] æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ...")
    try:
        with httpx.Client(timeout=5.0) as client:
            response = client.get(f"{base_url}/api/tags")
            response.raise_for_status()
            print("   âœ… Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ")
            
            # æ˜¾ç¤ºå·²å®‰è£…çš„æ¨¡å‹
            models = response.json().get("models", [])
            model_names = [m.get("name", "") for m in models]
            print(f"   ğŸ“¦ å·²å®‰è£…çš„æ¨¡å‹: {', '.join(model_names) if model_names else 'æ— '}")
            
    except httpx.ConnectError:
        print("   âŒ Ollama æœåŠ¡æœªè¿è¡Œ")
        print("   ğŸ’¡ å°è¯•è¿è¡Œ: brew services start ollama")
        issues.append("Ollama æœåŠ¡æœªè¿è¡Œ")
        return False, issues
    except Exception as e:
        print(f"   âŒ è¿æ¥å¤±è´¥: {e}")
        issues.append(f"è¿æ¥å¤±è´¥: {e}")
        return False, issues
    
    # æ­¥éª¤ 2: æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    print(f"\n[2/4] æ£€æŸ¥æ¨¡å‹ '{settings.ollama_model}' æ˜¯å¦å¯ç”¨...")
    try:
        with httpx.Client(timeout=5.0) as client:
            response = client.get(f"{base_url}/api/tags")
            models = response.json().get("models", [])
            model_names = [m.get("name", "") for m in models]
            
            if settings.ollama_model in model_names:
                print(f"   âœ… æ¨¡å‹ '{settings.ollama_model}' å·²å®‰è£…")
            else:
                print(f"   âŒ æ¨¡å‹ '{settings.ollama_model}' æœªå®‰è£…")
                print(f"   ğŸ’¡ è¿è¡Œ: ollama pull {settings.ollama_model}")
                issues.append(f"æ¨¡å‹ '{settings.ollama_model}' æœªå®‰è£…")
                return False, issues
    except Exception as e:
        print(f"   âŒ æ£€æŸ¥å¤±è´¥: {e}")
        issues.append(f"æ£€æŸ¥æ¨¡å‹å¤±è´¥: {e}")
        return False, issues
    
    # æ­¥éª¤ 3: æµ‹è¯•ç®€å•ç”Ÿæˆ
    print(f"\n[3/4] æµ‹è¯•æ¨¡å‹ç”ŸæˆåŠŸèƒ½...")
    try:
        with httpx.Client(timeout=30.0) as client:
            response = client.post(
                api_url,
                json={
                    "model": settings.ollama_model,
                    "prompt": "Say 'Hello' in one word",
                    "stream": False,
                }
            )
            response.raise_for_status()
            result = response.json()
            
            if "response" in result:
                generated_text = result["response"].strip()
                print(f"   âœ… ç”ŸæˆæˆåŠŸ")
                print(f"   ğŸ“ å“åº”: {generated_text[:50]}...")
            else:
                print(f"   âš ï¸  å“åº”æ ¼å¼å¼‚å¸¸")
                issues.append("å“åº”æ ¼å¼å¼‚å¸¸")
                return False, issues
    except httpx.HTTPStatusError as e:
        print(f"   âŒ HTTP é”™è¯¯: {e.response.status_code}")
        error_text = e.response.text[:200]
        print(f"   ğŸ“„ å“åº”: {error_text}")
        issues.append(f"HTTP {e.response.status_code}: {error_text}")
        return False, issues
    except Exception as e:
        print(f"   âŒ ç”Ÿæˆå¤±è´¥: {e}")
        issues.append(f"ç”Ÿæˆå¤±è´¥: {e}")
        return False, issues
    
    # æ­¥éª¤ 4: æµ‹è¯•å®Œæ•´æµç¨‹ï¼ˆæ¨¡æ‹Ÿåç«¯è°ƒç”¨ï¼‰
    print(f"\n[4/4] æµ‹è¯•å®Œæ•´ API è°ƒç”¨...")
    try:
        with httpx.Client(timeout=settings.ollama_timeout) as client:
            response = client.post(
                api_url,
                json={
                    "model": settings.ollama_model,
                    "prompt": "Test prompt",
                    "stream": False,
                    "options": {
                        "temperature": 0.8,
                        "num_predict": 10,
                    },
                }
            )
            response.raise_for_status()
            result = response.json()
            
            if "response" in result and result.get("done", False):
                print("   âœ… å®Œæ•´æµç¨‹æµ‹è¯•æˆåŠŸ")
            else:
                print("   âš ï¸  å“åº”ä¸å®Œæ•´")
                issues.append("å“åº”ä¸å®Œæ•´")
                return False, issues
    except Exception as e:
        print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
        issues.append(f"å®Œæ•´æµ‹è¯•å¤±è´¥: {e}")
        return False, issues
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Ollama é…ç½®æ­£å¸¸ã€‚")
    print("=" * 60)
    return True, []

if __name__ == "__main__":
    success, issues = check_ollama_service()
    if not success:
        print("\nâŒ å‘ç°é—®é¢˜ï¼Œè¯·æŒ‰ç…§ä¸Šè¿°æç¤ºè¿›è¡Œä¿®å¤ã€‚")
        if issues:
            print("\né—®é¢˜æ‘˜è¦:")
            for i, issue in enumerate(issues, 1):
                print(f"  {i}. {issue}")
    sys.exit(0 if success else 1)

