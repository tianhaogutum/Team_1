# 反馈惩罚机制说明

## 反馈惩罚机制

当用户对某个路线提交负面反馈（如"Too Difficult"）后，系统会对该路线应用**反馈惩罚**，根据反馈次数逐步降低推荐分数。

## 具体例子

### 场景1：用户反馈"Too Difficult"

**反馈前**：
- 路线A的推荐分数：0.80（80分，满分1.0）
- 路线A排名：第2名

**用户点击"👎 不喜欢" → 选择"Too Difficult" → 提交反馈**

**1次反馈后**：
- 路线A的推荐分数：0.80 × 0.5 = **0.40**（40分，降低到50%）
- 路线A排名：下降到第8名

**2次反馈后**（用户再次反馈）：
- 路线A的推荐分数：0.80 × 0.1 = **0.08**（8分，降低到10%）
- 路线A排名：下降到第18名

**3次反馈后**（用户第三次反馈）：
- 路线A的推荐分数：0.80 × 0.01 = **0.008**（0.8分，降低到1%）
- 路线A排名：下降到第25名

### 惩罚计算方式

```python
# 原始分数
original_score = 0.80

# 1次反馈：降低到50%
penalty_multiplier = 0.5  # 50%
final_score = original_score × penalty_multiplier
final_score = 0.80 × 0.5 = 0.40

# 2次反馈：降低到10%
penalty_multiplier = 0.1  # 10%
final_score = 0.80 × 0.1 = 0.08

# 3次反馈：降低到1%
penalty_multiplier = 0.01  # 1%
final_score = 0.80 × 0.01 = 0.008
```

## 多次反馈的惩罚

如果用户对同一条路线多次反馈，惩罚会更严重：

| 反馈次数 | 惩罚乘数 | 最终分数（原始0.80） | 说明 |
|---------|---------|-------------------|------|
| 0次 | 1.0（无惩罚） | 0.80 | 正常推荐 |
| 1次 | 0.5（50%） | 0.40 | 降低一半 |
| 2次 | 0.1（10%） | 0.08 | 大幅降低 |
| 3次 | 0.01（1%） | 0.008 | 几乎不推荐 |
| 4次+ | 完全过滤 | - | 不再显示 |

### 代码实现

```python
FEEDBACK_PENALTY_MULTIPLIERS = {
    1: 0.5,   # 50%
    2: 0.1,   # 10%
    3: 0.01,  # 1%
}

# 1次反馈：降低到50%
penalty = 0.5

# 2次反馈：降低到10%
penalty = 0.1

# 3次反馈：降低到1%
penalty = 0.01

# 4次反馈：完全过滤（不显示）
if feedback_count >= 4:
    # 不推荐这条路线
    continue
```

## 为什么是50%/10%/1%？

这个渐进式惩罚机制是一个平衡值：
- ✅ **1次反馈（50%）**：适度降低，给路线第二次机会
- ✅ **2次反馈（10%）**：明显降低优先级，但仍在列表中
- ✅ **3次反馈（1%）**：几乎不推荐，但保留在列表中
- ✅ **4次+反馈**：完全过滤，不再显示
- ✅ **保留学习机会**：系统仍可以从反馈中学习用户偏好

## 实际效果

### 用户视角

1. **提交反馈前**：
   ```
   推荐列表：
   1. 路线A (分数: 0.80) ⭐⭐⭐⭐
   2. 路线B (分数: 0.75) ⭐⭐⭐⭐
   3. 路线C (分数: 0.70) ⭐⭐⭐
   ```

2. **提交"Too Difficult"反馈后**（自动刷新）：
   ```
   推荐列表：
   1. 路线B (分数: 0.75) ⭐⭐⭐⭐  ← 上升
   2. 路线C (分数: 0.70) ⭐⭐⭐  ← 上升
   3. 路线A (分数: 0.40) ⭐⭐    ← 下降（1次反馈：50%）
   ```

3. **再次反馈后**（2次反馈）：
   ```
   推荐列表：
   1. 路线B (分数: 0.75) ⭐⭐⭐⭐
   2. 路线C (分数: 0.70) ⭐⭐⭐
   3. 路线A (分数: 0.08) ⭐      ← 大幅下降（2次反馈：10%）
   ```

### 系统视角

- **路线A的推荐分数**：
  - 1次反馈：从0.80降低到0.40（降低50%）
  - 2次反馈：从0.80降低到0.08（降低90%）
  - 3次反馈：从0.80降低到0.008（降低99%）
- **路线A的排名**：逐步下降
- **用户偏好调整**：系统降低用户的最大难度偏好
- **未来推荐**：更倾向于推荐难度较低的路线

## 完整流程

1. **用户操作**：
   - 看到路线推荐
   - 点击"👎 不喜欢"
   - 选择"💪 Too Difficult"
   - 点击"Submit Feedback"

2. **系统处理**：
   - 保存反馈到数据库
   - 应用5%惩罚：`score = original_score × 0.05`
   - 调整用户偏好：降低最大难度偏好
   - 重新计算所有路线的推荐分数
   - 重新排序推荐列表

3. **用户看到**：
   - 反馈对话框关闭
   - 推荐列表自动刷新
   - 被反馈的路线排名大幅下降
   - 其他路线排名上升

## 技术细节

### 后端代码

```python
# backend/app/services/recommendation_service.py

FEEDBACK_PENALTY_MULTIPLIERS = {
    1: 0.5,   # 50%
    2: 0.1,   # 10%
    3: 0.01,  # 1%
}

def calculate_feedback_penalty(route_id, feedback_entries):
    route_feedback = [f for f in feedback_entries if f.route_id == route_id]
    
    if not route_feedback:
        return 1.0  # 无惩罚
    
    feedback_count = len(route_feedback)
    
    # 根据反馈次数应用惩罚
    if feedback_count >= 3:
        return FEEDBACK_PENALTY_MULTIPLIERS[3]  # 1%
    elif feedback_count == 2:
        return FEEDBACK_PENALTY_MULTIPLIERS[2]  # 10%
    else:  # feedback_count == 1
        return FEEDBACK_PENALTY_MULTIPLIERS[1]  # 50%
```

### 应用惩罚

```python
# 计算基础CBF分数
base_score = 0.80

# 应用反馈惩罚
penalty = calculate_feedback_penalty(route_id, feedback_entries)
final_score = base_score × penalty

# 1次反馈：0.80 × 0.5 = 0.40
# 2次反馈：0.80 × 0.1 = 0.08
# 3次反馈：0.80 × 0.01 = 0.008
```

## 总结

**反馈惩罚机制**：
- **1次反馈**：推荐分数降低到原来的50%（例如：80分 → 40分）
- **2次反馈**：推荐分数降低到原来的10%（例如：80分 → 8分）
- **3次反馈**：推荐分数降低到原来的1%（例如：80分 → 0.8分）
- **4次+反馈**：完全过滤，不再显示

该路线在推荐列表中的排名会逐步下降，系统会学习用户偏好，未来推荐更符合用户需求的路线。

这样既尊重了用户的反馈，又保留了系统的学习能力，给路线提供了多次机会。

